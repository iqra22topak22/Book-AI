"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[8544],{6451:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"module-4-vision-language-action/lessons/lesson-1","title":"Lesson 4.1: Introduction to Voice Processing with Whisper","description":"Overview","source":"@site/docs/module-4-vision-language-action/lessons/lesson-1.md","sourceDirName":"module-4-vision-language-action/lessons","slug":"/module-4-vision-language-action/lessons/lesson-1","permalink":"/docs/module-4-vision-language-action/lessons/lesson-1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vision-language-action/lessons/lesson-1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Learning Outcomes for Module 4: Vision-Language-Action","permalink":"/docs/module-4-vision-language-action/learning-outcomes"},"next":{"title":"Lab 4.1: Voice Command Processing with Whisper","permalink":"/docs/module-4-vision-language-action/labs/lab-1"}}');var r=i(4848),o=i(8453);const t={},l="Lesson 4.1: Introduction to Voice Processing with Whisper",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Topics Covered",id:"topics-covered",level:2},{value:"1. Speech Recognition Fundamentals",id:"1-speech-recognition-fundamentals",level:2},{value:"Automatic Speech Recognition (ASR)",id:"automatic-speech-recognition-asr",level:3},{value:"Key Components of ASR Systems",id:"key-components-of-asr-systems",level:3},{value:"Challenges in Robot Applications",id:"challenges-in-robot-applications",level:3},{value:"2. OpenAI Whisper Architecture",id:"2-openai-whisper-architecture",level:2},{value:"Overview",id:"overview-1",level:3},{value:"Technical Architecture",id:"technical-architecture",level:3},{value:"Model Variants",id:"model-variants",level:3},{value:"Advantages for Robotics",id:"advantages-for-robotics",level:3},{value:"3. Voice Command Processing",id:"3-voice-command-processing",level:2},{value:"Command Structure",id:"command-structure",level:3},{value:"Processing Pipeline",id:"processing-pipeline",level:3},{value:"Real-time vs Batch Processing",id:"real-time-vs-batch-processing",level:3},{value:"4. Integration with Robotic Systems",id:"4-integration-with-robotic-systems",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Example ROS 2 Node Structure",id:"example-ros-2-node-structure",level:3},{value:"Microphone Array Integration",id:"microphone-array-integration",level:3},{value:"Command Validation",id:"command-validation",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Computing Requirements",id:"computing-requirements",level:3},{value:"Latency Optimization",id:"latency-optimization",level:3},{value:"Common Voice Commands in Robotics",id:"common-voice-commands-in-robotics",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-41-introduction-to-voice-processing-with-whisper",children:"Lesson 4.1: Introduction to Voice Processing with Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This lesson introduces voice processing in robotics using OpenAI's Whisper, a state-of-the-art automatic speech recognition (ASR) system. We'll explore how voice commands can be used to control robots and initiate complex actions."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the fundamentals of automatic speech recognition"}),"\n",(0,r.jsx)(n.li,{children:"Explain how Whisper works and its capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Set up Whisper for real-time voice processing"}),"\n",(0,r.jsx)(n.li,{children:"Integrate Whisper with ROS 2 for robot control"}),"\n",(0,r.jsx)(n.li,{children:"Process natural language commands for robotic tasks"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Speech Recognition Fundamentals"}),"\n",(0,r.jsx)(n.li,{children:"OpenAI Whisper Architecture"}),"\n",(0,r.jsx)(n.li,{children:"Voice Command Processing"}),"\n",(0,r.jsx)(n.li,{children:"Integration with Robotic Systems"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"1-speech-recognition-fundamentals",children:"1. Speech Recognition Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"automatic-speech-recognition-asr",children:"Automatic Speech Recognition (ASR)"}),"\n",(0,r.jsx)(n.p,{children:"Automatic Speech Recognition (ASR) is the technology that converts spoken language into text. In robotics, ASR enables natural human-robot interaction through voice commands."}),"\n",(0,r.jsx)(n.h3,{id:"key-components-of-asr-systems",children:"Key Components of ASR Systems"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Acoustic Model"}),": Maps audio signals to phonemes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language Model"}),": Predicts likely word sequences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Decoder"}),": Combines models to produce text output"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing"}),": Noise reduction, audio normalization"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"challenges-in-robot-applications",children:"Challenges in Robot Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise"}),": Mechanical and environmental noise"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distance"}),": Microphone placement and range"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accents"}),": Variability in pronunciation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": Processing latency requirements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context"}),": Understanding in specific environments"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2-openai-whisper-architecture",children:"2. OpenAI Whisper Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"overview-1",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is a robust speech recognition model that performs well across multiple languages and domains. It uses a multi-task approach to learn speech representations."}),"\n",(0,r.jsx)(n.h3,{id:"technical-architecture",children:"Technical Architecture"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Encoder"}),": Transformer-based encoder for audio processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Decoder"}),": Transformer-based decoder for text generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual"}),": Trained on 98+ languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robust"}),": Handles accents, background noise, technical speech"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-variants",children:"Model Variants"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tiny"}),": Fastest, smallest model (39M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"base"}),": Small model (74M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"small"}),": Medium model (244M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"medium"}),": Large model (769M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"large"}),": Largest model (1550M parameters)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"advantages-for-robotics",children:"Advantages for Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Works in various acoustic conditions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual"}),": Supports multiple languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contextual"}),": Can be fine-tuned for specific commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open-source"}),": Freely available for research and development"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"3-voice-command-processing",children:"3. Voice Command Processing"}),"\n",(0,r.jsx)(n.h3,{id:"command-structure",children:"Command Structure"}),"\n",(0,r.jsx)(n.p,{children:"Robot voice commands typically follow a structured format:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"[Action] + [Target] + [Parameters] + [Constraints]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Move to the kitchen" \u2192 Action: Move, Target: kitchen'}),"\n",(0,r.jsx)(n.li,{children:'"Pick up the red ball from the table" \u2192 Action: Pick up, Target: red ball, Location: table'}),"\n",(0,r.jsx)(n.li,{children:'"Navigate to the meeting room and wait for me" \u2192 Action: Navigate, Target: meeting room, Additional: wait'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"processing-pipeline",children:"Processing Pipeline"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Input"}),": Capture audio from microphone array"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing"}),": Filter, normalize, and segment audio"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transcription"}),": Convert speech to text using Whisper"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Natural Language Processing"}),": Parse text for intent"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command Mapping"}),": Map to robot actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execution"}),": Execute the command"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"real-time-vs-batch-processing",children:"Real-time vs Batch Processing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": Processes streaming audio, lower latency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch"}),": Processes complete utterances, higher accuracy"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"4-integration-with-robotic-systems",children:"4. Integration with Robotic Systems"}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,r.jsx)(n.p,{children:"Whisper can be integrated with ROS 2 in several ways:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Node-based"}),": Run Whisper as a dedicated ROS 2 node"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Service"}),": Provide transcription as a service"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": Handle long-running transcription tasks"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-ros-2-node-structure",children:"Example ROS 2 Node Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport whisper\nimport torch\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n        \n        # Initialize Whisper model\n        self.model = whisper.load_model(\"base\")\n        \n        # Setup subscribers and publishers\n        self.subscription = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.audio_callback,\n            10\n        )\n        \n        self.publisher = self.create_publisher(\n            String,\n            'transcribed_text',\n            10\n        )\n    \n    def audio_callback(self, msg):\n        # Process audio data\n        audio_array = self.process_audio(msg.data)\n        \n        # Transcribe using Whisper\n        result = self.model.transcribe(audio_array)\n        text = result['text']\n        \n        # Publish transcribed text\n        transcribed_msg = String()\n        transcribed_msg.data = text\n        self.publisher.publish(transcribed_msg)\n        \n        self.get_logger().info(f'Transcribed: {text}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"microphone-array-integration",children:"Microphone Array Integration"}),"\n",(0,r.jsx)(n.p,{children:"For robotics applications, microphone arrays are preferred for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Reduction"}),": Spatial filtering of background noise"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Directional Sensing"}),": Focus on speaker direction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Echo Cancellation"}),": Remove robot-generated noise"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"command-validation",children:"Command Validation"}),"\n",(0,r.jsx)(n.p,{children:"Voice commands should be validated against:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context"}),": Is the command appropriate for the current situation?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),": Does the command pose any safety risks?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feasibility"}),": Can the robot physically execute the command?"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"computing-requirements",children:"Computing Requirements"}),"\n",(0,r.jsx)(n.p,{children:"Whisper models require significant computational resources:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tiny/base"}),": Can run on modern CPUs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"small/medium/large"}),": GPU acceleration recommended"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": Depends on model size and audio length"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Choice"}),": Balance accuracy vs. speed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": Use GPU acceleration where possible"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Chunking"}),": Process audio in smaller segments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Caching"}),": Cache frequently used transcriptions"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"common-voice-commands-in-robotics",children:"Common Voice Commands in Robotics"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Navigation"}),': "Go to the kitchen", "Move forward", "Turn left"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Manipulation"}),': "Pick up the object", "Place in the box", "Open the door"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interaction"}),': "Follow me", "Stop", "Wait here"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Information"}),': "What do you see?", "Where am I?"']}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Clear Commands"}),": Use simple, unambiguous language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feedback"}),": Provide audio or visual confirmation of received commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Confirmation"}),": Ask for confirmation on complex or dangerous commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fallback"}),": Have alternative input methods when voice fails"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Consider privacy implications of voice processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,r.jsx)(n.p,{children:"Research and compare Whisper with other ASR systems like Google Speech-to-Text API, Microsoft Azure Speech, and Mozilla DeepSpeech in terms of:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accuracy for robotics commands"}),"\n",(0,r.jsx)(n.li,{children:"Real-time processing capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Privacy considerations"}),"\n",(0,r.jsx)(n.li,{children:"Offline operation capability"}),"\n",(0,r.jsx)(n.li,{children:"Computing requirements"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Whisper provides a powerful foundation for voice-based robot control, offering robust speech recognition capabilities that can be integrated with ROS 2 systems. Its multilingual support and noise robustness make it well-suited for robotic applications, though computing requirements and latency considerations must be carefully managed. The combination of voice processing with natural language understanding enables more natural human-robot interaction."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(6540);const r={},o=s.createContext(r);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);