"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[1786],{8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(6540);const r={},t=i.createContext(r);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:n},e.children)}},9461:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vision-language-action/labs/lab-1","title":"Lab 4.1: Voice Command Processing with Whisper","description":"Overview","source":"@site/docs/module-4-vision-language-action/labs/lab-1.md","sourceDirName":"module-4-vision-language-action/labs","slug":"/module-4-vision-language-action/labs/lab-1","permalink":"/docs/module-4-vision-language-action/labs/lab-1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vision-language-action/labs/lab-1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1: Introduction to Voice Processing with Whisper","permalink":"/docs/module-4-vision-language-action/lessons/lesson-1"},"next":{"title":"Module 4 Deliverables: Vision-Language-Action","permalink":"/docs/module-4-vision-language-action/deliverables/deliverable-1"}}');var r=s(4848),t=s(8453);const o={},a="Lab 4.1: Voice Command Processing with Whisper",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lab Duration",id:"lab-duration",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Verify Whisper Installation",id:"step-1-verify-whisper-installation",level:3},{value:"Step 2: Install Additional Dependencies",id:"step-2-install-additional-dependencies",level:3},{value:"Step 3: Create the Whisper Node Package",id:"step-3-create-the-whisper-node-package",level:3},{value:"Step 4: Create the Whisper Node",id:"step-4-create-the-whisper-node",level:3},{value:"Step 5: Create Audio Input Node",id:"step-5-create-audio-input-node",level:3},{value:"Step 6: Create Voice Command Parser Node",id:"step-6-create-voice-command-parser-node",level:3},{value:"Step 7: Update setup.py",id:"step-7-update-setuppy",level:3},{value:"Step 8: Build and Test",id:"step-8-build-and-test",level:3},{value:"Step 9: Start Voice Recording",id:"step-9-start-voice-recording",level:3},{value:"Step 10: Monitor Output",id:"step-10-monitor-output",level:3},{value:"Step 11: Experiment and Extend",id:"step-11-experiment-and-extend",level:3},{value:"Deliverable",id:"deliverable",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lab-41-voice-command-processing-with-whisper",children:"Lab 4.1: Voice Command Processing with Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This lab provides hands-on experience with voice command processing using OpenAI's Whisper and integrating it with a robotic system. You'll create a system that receives voice commands, processes them with Whisper, and executes appropriate actions."}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble Hawksbill installed"}),"\n",(0,r.jsx)(n.li,{children:"OpenAI Whisper installed"}),"\n",(0,r.jsx)(n.li,{children:"Audio input device (microphone or microphone array)"}),"\n",(0,r.jsx)(n.li,{children:"Basic understanding of voice processing concepts from Lesson 4.1"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lab, students will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Set up Whisper for voice processing in ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"Create a voice command processing pipeline"}),"\n",(0,r.jsx)(n.li,{children:"Integrate voice processing with robot control"}),"\n",(0,r.jsx)(n.li,{children:"Implement command validation and safety checks"}),"\n",(0,r.jsx)(n.li,{children:"Process natural language robot commands"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"lab-duration",children:"Lab Duration"}),"\n",(0,r.jsx)(n.p,{children:"Estimated time: 4-5 hours"}),"\n",(0,r.jsx)(n.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-verify-whisper-installation",children:"Step 1: Verify Whisper Installation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Check if Whisper is installed:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 -c \"import whisper; print('Whisper version:', whisper.__version__)\"\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"If not installed, install Whisper:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install openai-whisper\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"For GPU acceleration, ensure you have PyTorch with CUDA:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-install-additional-dependencies",children:"Step 2: Install Additional Dependencies"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Install audio processing libraries:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install pyaudio soundfile librosa\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Install ROS 2 audio packages:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sudo apt install ros-humble-audio-common-msgs\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-create-the-whisper-node-package",children:"Step 3: Create the Whisper Node Package"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create a new ROS 2 package for voice processing:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/physical_ai_ws/src\nros2 pkg create --build-type ament_python voice_command_processing\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Update the package.xml with dependencies:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>voice_command_processing</name>\n  <version>0.0.0</version>\n  <description>Package for voice command processing with Whisper</description>\n  <maintainer email="your_email@example.com">your_name</maintainer>\n  <license>TODO: License declaration</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>audio_common_msgs</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-create-the-whisper-node",children:"Step 4: Create the Whisper Node"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["In the package directory, create the main Whisper node file ",(0,r.jsx)(n.code,{children:"voice_command_processing/whisper_node.py"}),":"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport whisper\nimport torch\nimport numpy as np\nimport pyaudio\nimport wave\nimport threading\nimport queue\nfrom std_msgs.msg import String, Bool\nfrom audio_common_msgs.msg import AudioData\nimport json\n\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n        \n        # Check for CUDA availability\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.get_logger().info(f"Using device: {self.device}")\n        \n        # Load Whisper model (using \'tiny\' for faster processing in lab)\n        self.get_logger().info("Loading Whisper model...")\n        self.model = whisper.load_model("tiny", device=self.device)\n        self.get_logger().info("Whisper model loaded successfully")\n        \n        # Initialize audio processing\n        self.audio_queue = queue.Queue()\n        self.recording = False\n        self.sample_rate = 16000  # Standard for Whisper\n        self.chunk_size = 1024\n        \n        # Setup subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio_input\',\n            self.audio_callback,\n            10\n        )\n        \n        self.transcription_pub = self.create_publisher(\n            String,\n            \'transcribed_text\',\n            10\n        )\n        \n        self.command_pub = self.create_publisher(\n            String,\n            \'parsed_command\',\n            10\n        )\n        \n        self.recording_status_pub = self.create_publisher(\n            Bool,\n            \'recording_status\',\n            10\n        )\n        \n        # Setup recording service\n        self.start_recording_service = self.create_service(\n            Bool,\n            \'start_voice_recording\',\n            self.start_recording_callback\n        )\n        \n        self.stop_recording_service = self.create_service(\n            Bool,\n            \'stop_voice_recording\',\n            self.stop_recording_callback\n        )\n        \n        # Start audio processing thread\n        self.audio_thread = threading.Thread(target=self.process_audio)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n        \n        self.get_logger().info("Whisper Node initialized")\n\n    def audio_callback(self, msg):\n        """Callback for audio input from ROS topic"""\n        # Convert byte data to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n        self.audio_queue.put(audio_data)\n\n    def start_recording_callback(self, request, response):\n        """Start voice recording service"""\n        self.recording = True\n        status_msg = Bool()\n        status_msg.data = True\n        self.recording_status_pub.publish(status_msg)\n        self.get_logger().info("Voice recording started")\n        response.data = True\n        return response\n\n    def stop_recording_callback(self, request, response):\n        """Stop voice recording service"""\n        self.recording = False\n        status_msg = Bool()\n        status_msg.data = False\n        self.recording_status_pub.publish(status_msg)\n        self.get_logger().info("Voice recording stopped")\n        response.data = True\n        return response\n\n    def process_audio(self):\n        """Process audio chunks in a separate thread"""\n        audio_buffer = np.array([])\n        \n        while rclpy.ok():\n            try:\n                # Get audio chunk from queue\n                chunk = self.audio_queue.get(timeout=0.1)\n                \n                if self.recording:\n                    # Add chunk to buffer\n                    audio_buffer = np.concatenate([audio_buffer, chunk])\n                    \n                    # If buffer is large enough, transcribe\n                    if len(audio_buffer) >= self.sample_rate * 2:  # At least 2 seconds\n                        # Transcribe the audio\n                        transcription = self.transcribe_audio(audio_buffer)\n                        \n                        if transcription.strip():  # If transcription is not empty\n                            # Publish transcribed text\n                            transcribed_msg = String()\n                            transcribed_msg.data = transcription\n                            self.transcription_pub.publish(transcribed_msg)\n                            \n                            # Parse and publish command\n                            command = self.parse_command(transcription)\n                            if command:\n                                command_msg = String()\n                                command_msg.data = command\n                                self.command_pub.publish(command_msg)\n                                self.get_logger().info(f\'Command: {command}\')\n                        \n                        # Reset buffer after transcription\n                        audio_buffer = np.array([])\n            except queue.Empty:\n                continue  # Continue if queue is empty\n\n    def transcribe_audio(self, audio_data):\n        """Transcribe audio using Whisper"""\n        try:\n            # Convert to 16kHz if needed\n            if len(audio_data) == 0:\n                return ""\n                \n            # Pad if too short (Whisper expects at least 0.1 seconds)\n            if len(audio_data) < self.sample_rate * 0.1:\n                padding = int(self.sample_rate * 0.1) - len(audio_data)\n                audio_data = np.pad(audio_data, (0, padding), mode=\'constant\')\n            \n            # Transcribe using Whisper\n            result = self.model.transcribe(audio_data, fp16=False if self.device == \'cpu\' else True)\n            return result[\'text\'].strip()\n        except Exception as e:\n            self.get_logger().error(f"Error in transcription: {e}")\n            return ""\n\n    def parse_command(self, text):\n        """Parse natural language command and convert to robot command"""\n        text = text.lower().strip()\n        \n        # Define simple command patterns\n        if "move forward" in text or "go forward" in text or "move ahead" in text:\n            return "MOVE_FORWARD"\n        elif "move backward" in text or "go back" in text or "reverse" in text:\n            return "MOVE_BACKWARD"\n        elif "turn left" in text or "rotate left" in text:\n            return "TURN_LEFT"\n        elif "turn right" in text or "rotate right" in text:\n            return "TURN_RIGHT"\n        elif "stop" in text or "halt" in text:\n            return "STOP"\n        elif "pick up" in text or "grasp" in text or "grab" in text:\n            return "PICK_UP_OBJECT"\n        elif "put down" in text or "place" in text or "release" in text:\n            return "PUT_DOWN_OBJECT"\n        elif "come to me" in text or "follow me" in text:\n            return "FOLLOW_ME"\n        elif "go to" in text or "navigate to" in text:\n            # Extract location if possible\n            if "kitchen" in text:\n                return "NAVIGATE_TO_KITCHEN"\n            elif "living room" in text:\n                return "NAVIGATE_TO_LIVING_ROOM"\n            elif "bedroom" in text:\n                return "NAVIGATE_TO_BEDROOM"\n            else:\n                return "NAVIGATE_TO_LOCATION"\n        else:\n            # Return the original text if no recognized command\n            return f"UNKNOWN_COMMAND: {text}"\n\n    def destroy_node(self):\n        """Clean up when node is destroyed"""\n        self.recording = False\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-5-create-audio-input-node",children:"Step 5: Create Audio Input Node"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Create an audio input node ",(0,r.jsx)(n.code,{children:"voice_command_processing/audio_input_node.py"}),":"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport pyaudio\nimport numpy as np\nfrom audio_common_msgs.msg import AudioData\n\n\nclass AudioInputNode(Node):\n    def __init__(self):\n        super().__init__(\'audio_input_node\')\n        \n        # Setup audio parameters\n        self.rate = 16000  # Whisper standard\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        \n        # Setup publisher\n        self.publisher = self.create_publisher(AudioData, \'audio_input\', 10)\n        \n        # Setup PyAudio\n        self.audio = pyaudio.PyAudio()\n        \n        # Start audio stream\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        # Timer to periodically publish audio data\n        self.timer = self.create_timer(0.1, self.publish_audio)  # 100ms interval\n        \n        self.get_logger().info("Audio Input Node initialized")\n\n    def publish_audio(self):\n        """Read audio data and publish to topic"""\n        try:\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\n            \n            # Create and publish audio message\n            msg = AudioData()\n            msg.data = data\n            self.publisher.publish(msg)\n            \n        except Exception as e:\n            self.get_logger().error(f"Error reading audio: {e}")\n\n    def destroy_node(self):\n        """Clean up when node is destroyed"""\n        if hasattr(self, \'stream\'):\n            self.stream.stop_stream()\n            self.stream.close()\n        if hasattr(self, \'audio\'):\n            self.audio.terminate()\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AudioInputNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-6-create-voice-command-parser-node",children:"Step 6: Create Voice Command Parser Node"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Create a command parsing node ",(0,r.jsx)(n.code,{children:"voice_command_processing/command_parser_node.py"}),":"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\n\n\nclass CommandParserNode(Node):\n    def __init__(self):\n        super().__init__(\'command_parser_node\')\n        \n        # Setup subscriber for parsed commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'parsed_command\',\n            self.command_callback,\n            10\n        )\n        \n        # Setup publisher for robot control commands\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        \n        self.get_logger().info("Command Parser Node initialized")\n\n    def command_callback(self, msg):\n        """Process parsed commands and generate robot control commands"""\n        command = msg.data.upper()\n        self.get_logger().info(f"Processing command: {command}")\n        \n        # Create Twist message for robot movement\n        twist = Twist()\n        \n        if command == "MOVE_FORWARD":\n            twist.linear.x = 0.5  # Move forward at 0.5 m/s\n        elif command == "MOVE_BACKWARD":\n            twist.linear.x = -0.5  # Move backward at 0.5 m/s\n        elif command == "TURN_LEFT":\n            twist.angular.z = 0.5  # Turn left at 0.5 rad/s\n        elif command == "TURN_RIGHT":\n            twist.angular.z = -0.5  # Turn right at 0.5 rad/s\n        elif command == "STOP":\n            # Twist is already zero, so robot will stop\n            pass\n        else:\n            self.get_logger().info(f"Command not mapped to robot action: {command}")\n            return  # Don\'t publish if command isn\'t mapped to movement\n        \n        # Publish the command\n        self.cmd_vel_pub.publish(twist)\n        self.get_logger().info(f"Published command: linear.x={twist.linear.x}, angular.z={twist.angular.z}")\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandParserNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-7-update-setuppy",children:"Step 7: Update setup.py"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Update the package's ",(0,r.jsx)(n.code,{children:"setup.py"})," to include entry points:"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'voice_command_processing'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='your_name',\n    maintainer_email='your_email@example.com',\n    description='Package for voice command processing with Whisper',\n    license='TODO: License declaration',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'whisper_node = voice_command_processing.whisper_node:main',\n            'audio_input_node = voice_command_processing.audio_input_node:main',\n            'command_parser_node = voice_command_processing.command_parser_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-8-build-and-test",children:"Step 8: Build and Test"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Build the workspace:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/physical_ai_ws\ncolcon build --packages-select voice_command_processing\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Source the workspace:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Test the audio input node separately first:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run voice_command_processing audio_input_node\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsx)(n.li,{children:"In a new terminal, run the Whisper node:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\nros2 run voice_command_processing whisper_node\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsx)(n.li,{children:"In another terminal, run the command parser node:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\nros2 run voice_command_processing command_parser_node\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"6",children:["\n",(0,r.jsx)(n.li,{children:"Test by publishing voice commands or using services to control recording."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-9-start-voice-recording",children:"Step 9: Start Voice Recording"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Use the service to start recording:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'ros2 service call /start_voice_recording std_msgs/srv/Bool "{data: true}"\n'})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Speak commands to your microphone."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Use the service to stop recording when done:"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'ros2 service call /stop_voice_recording std_msgs/srv/Bool "{data: false}"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-10-monitor-output",children:"Step 10: Monitor Output"}),"\n",(0,r.jsx)(n.p,{children:"Monitor the transcribed text and parsed commands:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Monitor transcribed text\nros2 topic echo /transcribed_text\n\n# Monitor parsed commands\nros2 topic echo /parsed_command\n\n# Monitor robot commands (if connected to a robot)\nros2 topic echo /cmd_vel\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-11-experiment-and-extend",children:"Step 11: Experiment and Extend"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Modify the command parsing logic to recognize more commands"}),"\n",(0,r.jsx)(n.li,{children:"Add safety checks to prevent dangerous commands"}),"\n",(0,r.jsx)(n.li,{children:"Improve the audio preprocessing for better quality"}),"\n",(0,r.jsx)(n.li,{children:"Adjust the Whisper model (try 'base' for better accuracy)"}),"\n",(0,r.jsx)(n.li,{children:"Add confidence scoring to filter uncertain transcriptions"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"deliverable",children:"Deliverable"}),"\n",(0,r.jsx)(n.p,{children:"Submit a report containing:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Screenshots of the running voice command system"}),"\n",(0,r.jsx)(n.li,{children:"Examples of voice commands and their interpretations"}),"\n",(0,r.jsx)(n.li,{children:"Modified code that demonstrates at least one extension from Step 11"}),"\n",(0,r.jsx)(n.li,{children:"Analysis of system accuracy and performance"}),"\n",(0,r.jsx)(n.li,{children:"Video demonstration of voice-controlled robot operation"}),"\n",(0,r.jsx)(n.li,{children:"Discussion of challenges encountered and solutions implemented"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper integration with ROS 2 (25%)"}),"\n",(0,r.jsx)(n.li,{children:"Voice command processing pipeline (25%)"}),"\n",(0,r.jsx)(n.li,{children:"Command parsing and robot control (20%)"}),"\n",(0,r.jsx)(n.li,{children:"System robustness and error handling (15%)"}),"\n",(0,r.jsx)(n.li,{children:"Report quality and analysis (15%)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio device not found"}),": Verify microphone is connected and accessible"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CUDA errors"}),": Ensure PyTorch with CUDA is installed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Whisper loading errors"}),": Check internet connection for model download"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"High latency"}),": Use smaller Whisper model or optimize audio processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recognition issues"}),": Ensure quiet environment and clear speech"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://index.ros.org/p/audio_common/",children:"ROS 2 Audio Processing"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://pyaudio.readthedocs.io/",children:"PyAudio Documentation"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);