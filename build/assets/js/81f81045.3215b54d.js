"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[7542],{6438:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"guides/capstone-guide","title":"Capstone Project Guide: Voice-Controlled Robot Assistant","description":"Overview","source":"@site/docs/guides/capstone-guide.md","sourceDirName":"guides","slug":"/guides/capstone-guide","permalink":"/docs/guides/capstone-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/guides/capstone-guide.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 and Isaac Components Reference","permalink":"/docs/reference/ros-isaac-components"},"next":{"title":"Instructor Resources","permalink":"/docs/instructors/overview"}}');var s=i(4848),a=i(8453);const o={sidebar_position:10},r="Capstone Project Guide: Voice-Controlled Robot Assistant",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Project Structure",id:"project-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Phase 1: System Architecture Design",id:"phase-1-system-architecture-design",level:2},{value:"1.1 High-Level Architecture",id:"11-high-level-architecture",level:3},{value:"1.2 Component Integration",id:"12-component-integration",level:3},{value:"1.3 Communication Architecture",id:"13-communication-architecture",level:3},{value:"Phase 2: Voice Processing Implementation",id:"phase-2-voice-processing-implementation",level:2},{value:"2.1 Whisper Integration",id:"21-whisper-integration",level:3},{value:"2.2 Natural Language Processing",id:"22-natural-language-processing",level:3},{value:"Phase 3: Planning System",id:"phase-3-planning-system",level:2},{value:"3.1 LLM-Based Planning",id:"31-llm-based-planning",level:3},{value:"Phase 4: Navigation Implementation",id:"phase-4-navigation-implementation",level:2},{value:"4.1 Nav2 Integration",id:"41-nav2-integration",level:3},{value:"4.2 Isaac Sim Integration",id:"42-isaac-sim-integration",level:3},{value:"Phase 5: Perception System",id:"phase-5-perception-system",level:2},{value:"5.1 Object Detection",id:"51-object-detection",level:3},{value:"5.2 Isaac ROS Perception",id:"52-isaac-ros-perception",level:3},{value:"Phase 6: Manipulation Implementation",id:"phase-6-manipulation-implementation",level:2},{value:"6.1 Robot Control",id:"61-robot-control",level:3},{value:"Phase 7: Complete Integration",id:"phase-7-complete-integration",level:2},{value:"7.1 System Composition",id:"71-system-composition",level:3},{value:"7.2 Safety and Error Handling",id:"72-safety-and-error-handling",level:3},{value:"Phase 8: Testing and Validation",id:"phase-8-testing-and-validation",level:2},{value:"8.1 Individual Component Testing",id:"81-individual-component-testing",level:3},{value:"8.2 Integrated Testing",id:"82-integrated-testing",level:3},{value:"8.3 Scenario Testing",id:"83-scenario-testing",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Strategies",id:"debugging-strategies",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Real-time Performance",id:"1-real-time-performance",level:3},{value:"2. Resource Management",id:"2-resource-management",level:3},{value:"3. Communication Efficiency",id:"3-communication-efficiency",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Technical Requirements",id:"technical-requirements",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"capstone-project-guide-voice-controlled-robot-assistant",children:"Capstone Project Guide: Voice-Controlled Robot Assistant"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This guide provides detailed instructions for completing the capstone project, where you'll implement the complete Voice \u2192 Plan \u2192 Navigate \u2192 Perceive \u2192 Manipulate workflow using the concepts learned throughout the course."}),"\n",(0,s.jsx)(n.h2,{id:"project-structure",children:"Project Structure"}),"\n",(0,s.jsx)(n.p,{children:"The capstone project integrates all four course modules into a comprehensive system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice"}),": Process natural language commands using Whisper"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan"}),": Generate action sequences using LLM-based planning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigate"}),": Execute navigation tasks using Nav2 and Isaac"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perceive"}),": Detect and identify objects using perception systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulate"}),": Perform physical actions on identified objects"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting the capstone project, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed all four course modules"}),"\n",(0,s.jsx)(n.li,{children:"Set up the necessary hardware (Unitree robot or simulation)"}),"\n",(0,s.jsx)(n.li,{children:"Installed all required software components"}),"\n",(0,s.jsx)(n.li,{children:"Successfully tested individual components from each module"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-1-system-architecture-design",children:"Phase 1: System Architecture Design"}),"\n",(0,s.jsx)(n.h3,{id:"11-high-level-architecture",children:"1.1 High-Level Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Design the overall system architecture by connecting components from all modules:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Voice Command] \n      \u2193\n[Natural Language Processing] \n      \u2193\n[Task Planner] \n      \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Navigation \u2500\u2192 Perception \u2500\u2192 Manipulation \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2193\n[Execution & Feedback]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"12-component-integration",children:"1.2 Component Integration"}),"\n",(0,s.jsx)(n.p,{children:"Create a component diagram showing how all modules integrate:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Whisper for voice processing"}),"\n",(0,s.jsx)(n.li,{children:"LLM for task planning"}),"\n",(0,s.jsx)(n.li,{children:"Nav2 for navigation"}),"\n",(0,s.jsx)(n.li,{children:"Isaac Sim/ROS for perception"}),"\n",(0,s.jsx)(n.li,{children:"Hardware controllers for manipulation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"13-communication-architecture",children:"1.3 Communication Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Define how all components communicate:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 topics for real-time sensor/control data"}),"\n",(0,s.jsx)(n.li,{children:"Services for request-response operations"}),"\n",(0,s.jsx)(n.li,{children:"Actions for goal-oriented behaviors"}),"\n",(0,s.jsx)(n.li,{children:"Transform trees for spatial relationships"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-2-voice-processing-implementation",children:"Phase 2: Voice Processing Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"21-whisper-integration",children:"2.1 Whisper Integration"}),"\n",(0,s.jsx)(n.p,{children:"Implement the voice processing component:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport torch\n\nclass VoiceProcessorNode(Node):\n    def __init__(self):\n        super().__init__('voice_processor')\n        \n        # Initialize Whisper model\n        self.model = whisper.load_model(\"base\")\n        \n        # Publishers and subscribers\n        self.voice_sub = self.create_subscription(\n            String, \n            'voice_input', \n            self.voice_callback, \n            10\n        )\n        \n        self.command_pub = self.create_publisher(\n            String, \n            'natural_language_command', \n            10\n        )\n    \n    def voice_callback(self, msg):\n        # Process audio through Whisper\n        result = self.model.transcribe(msg.data)\n        \n        # Publish transcribed command\n        command_msg = String()\n        command_msg.data = result['text']\n        self.command_pub.publish(command_msg)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"22-natural-language-processing",children:"2.2 Natural Language Processing"}),"\n",(0,s.jsx)(n.p,{children:"Convert natural language commands to structured actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def parse_command(self, text):\n    text = text.lower()\n    \n    # Define command patterns\n    if "go to" in text or "navigate to" in text:\n        return self.extract_navigation_command(text)\n    elif "pick up" in text or "grab" in text:\n        return self.extract_manipulation_command(text)\n    elif "perceive" in text or "find" in text:\n        return self.extract_perception_command(text)\n    else:\n        return {"action": "unknown", "details": text}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"phase-3-planning-system",children:"Phase 3: Planning System"}),"\n",(0,s.jsx)(n.h3,{id:"31-llm-based-planning",children:"3.1 LLM-Based Planning"}),"\n",(0,s.jsx)(n.p,{children:"Implement the planning component to decompose high-level commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PlannerNode(Node):\n    def __init__(self):\n        super().__init__('planner')\n        \n        self.command_sub = self.create_subscription(\n            String,\n            'natural_language_command',\n            self.command_callback,\n            10\n        )\n        \n        self.plan_pub = self.create_publisher(\n            Plan,\n            'execution_plan',\n            10\n        )\n    \n    def command_callback(self, msg):\n        # Use LLM to decompose command\n        plan = self.generate_execution_plan(msg.data)\n        self.plan_pub.publish(plan)\n    \n    def generate_execution_plan(self, command):\n        # Use an LLM to decompose high-level commands\n        # into actionable steps\n        prompt = f\"\"\"\n        Decompose this robot command into sequential steps:\n        Command: {command}\n        \n        Provide the steps in the following format:\n        1. Navigation: [location]\n        2. Perception: [what to look for]\n        3. Manipulation: [what action to perform]\n        \"\"\"\n        \n        # Execute LLM call here\n        # Implementation depends on your chosen LLM\n        response = self.llm_call(prompt)\n        return self.parse_llm_response(response)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-4-navigation-implementation",children:"Phase 4: Navigation Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"41-nav2-integration",children:"4.1 Nav2 Integration"}),"\n",(0,s.jsx)(n.p,{children:"Set up navigation with Nav2:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from nav2_simple_commander.robot_navigator import BasicNavigator\nimport rclpy\n\nclass NavigationNode(Node):\n    def __init__(self):\n        super().__init__('navigation_node')\n        self.navigator = BasicNavigator()\n        \n        # Wait for Nav2 to be ready\n        self.navigator.waitUntilNav2Active()\n        \n        self.plan_sub = self.create_subscription(\n            Plan,\n            'execution_plan',\n            self.plan_callback,\n            10\n        )\n    \n    def plan_callback(self, plan_msg):\n        # Extract navigation goal from plan\n        goal = self.extract_navigation_goal(plan_msg)\n        \n        # Navigate to goal\n        self.navigator.goToPose(goal)\n        \n        # Wait for completion\n        while not self.navigator.isTaskComplete():\n            feedback = self.navigator.getFeedback()\n            if feedback:\n                self.get_logger().info(f'Navigating: {feedback.distance_remaining:.2f}m remaining')\n        \n        result = self.navigator.getResult()\n        if result == TaskResult.SUCCEEDED:\n            self.get_logger().info('Navigation succeeded!')\n        else:\n            self.get_logger().error('Navigation failed!')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"42-isaac-sim-integration",children:"4.2 Isaac Sim Integration"}),"\n",(0,s.jsx)(n.p,{children:"For simulation, ensure Isaac Sim is properly configured:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up navigation maps in Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with perception for dynamic obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Configure navigation parameters for your environment"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-5-perception-system",children:"Phase 5: Perception System"}),"\n",(0,s.jsx)(n.h3,{id:"51-object-detection",children:"5.1 Object Detection"}),"\n",(0,s.jsx)(n.p,{children:"Implement perception to identify targets:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n        \n        # Subscribe to camera data\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        # Publisher for detected objects\n        self.objects_pub = self.create_publisher(\n            ObjectList,\n            'detected_objects',\n            10\n        )\n        \n        # Initialize object detector\n        self.detector = self.initialize_detector()\n    \n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        \n        # Perform object detection\n        detections = self.detector.detect(cv_image)\n        \n        # Publish results\n        object_list = self.create_object_list(detections)\n        self.objects_pub.publish(object_list)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"52-isaac-ros-perception",children:"5.2 Isaac ROS Perception"}),"\n",(0,s.jsx)(n.p,{children:"Integrate with Isaac ROS perception packages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example using Isaac ROS DetectNet for object detection\nfrom isaac_ros_detect_net_interfaces.msg import Detection2DArray\n\nclass IsaacPerceptionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_node')\n        \n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            '/detectnet/detections',\n            self.detection_callback,\n            10\n        )\n    \n    def detection_callback(self, msg):\n        for detection in msg.detections:\n            if detection.results[0].id == TARGET_OBJECT_ID:\n                # Target object detected\n                self.handle_target_detection(detection)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-6-manipulation-implementation",children:"Phase 6: Manipulation Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"61-robot-control",children:"6.1 Robot Control"}),"\n",(0,s.jsx)(n.p,{children:"Implement manipulation based on perception results:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ManipulationNode(Node):\n    def __init__(self):\n        super().__init__('manipulation_node')\n        \n        self.objects_sub = self.create_subscription(\n            ObjectList,\n            'detected_objects',\n            self.objects_callback,\n            10\n        )\n        \n        # Initialize manipulator controller\n        self.arm_controller = self.initialize_arm_controller()\n    \n    def objects_callback(self, msg):\n        for obj in msg.objects:\n            if obj.name == self.target_object:\n                # Calculate grasp pose\n                grasp_pose = self.calculate_grasp_pose(obj)\n                \n                # Execute manipulation\n                success = self.arm_controller.grasp(grasp_pose)\n                \n                if success:\n                    self.get_logger().info(f'Successfully grasped {obj.name}')\n                else:\n                    self.get_logger().error(f'Failed to grasp {obj.name}')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"phase-7-complete-integration",children:"Phase 7: Complete Integration"}),"\n",(0,s.jsx)(n.h3,{id:"71-system-composition",children:"7.1 System Composition"}),"\n",(0,s.jsx)(n.p,{children:"Create a launch file that brings up all components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Voice processing\n        Node(\n            package='capstone_project',\n            executable='voice_processor',\n            name='voice_processor'\n        ),\n        \n        # Planner\n        Node(\n            package='capstone_project',\n            executable='planner',\n            name='planner'\n        ),\n        \n        # Navigation\n        Node(\n            package='capstone_project',\n            executable='navigation',\n            name='navigation'\n        ),\n        \n        # Perception\n        Node(\n            package='capstone_project',\n            executable='perception',\n            name='perception'\n        ),\n        \n        # Manipulation\n        Node(\n            package='capstone_project',\n            executable='manipulation',\n            name='manipulation'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"72-safety-and-error-handling",children:"7.2 Safety and Error Handling"}),"\n",(0,s.jsx)(n.p,{children:"Implement safety checks throughout the system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CapstoneSystemManager(Node):\n    def __init__(self):\n        super().__init__(\'capstone_manager\')\n        \n        # Safety parameters\n        self.max_navigation_attempts = 3\n        self.perception_timeout = 10.0  # seconds\n        self.emergency_stop = False\n        \n        # System state tracker\n        self.current_phase = "idle"\n        \n        # Initialize all subsystems\n        self.initialize_subsystems()\n    \n    def execute_command(self, command):\n        try:\n            if self.emergency_stop:\n                return False\n                \n            # Phase 1: Process voice command\n            self.current_phase = "voice_processing"\n            nl_command = self.process_voice_command(command)\n            \n            # Phase 2: Plan actions\n            self.current_phase = "planning"\n            plan = self.generate_plan(nl_command)\n            \n            # Phase 3: Navigate\n            self.current_phase = "navigation"\n            nav_success = self.execute_navigation(plan.nav_goal)\n            if not nav_success:\n                self.get_logger().error("Navigation failed")\n                return False\n            \n            # Phase 4: Perceive\n            self.current_phase = "perception"\n            objects = self.perform_perception(plan.perception_task)\n            if not objects:\n                self.get_logger().error("No target objects detected")\n                return False\n            \n            # Phase 5: Manipulate\n            self.current_phase = "manipulation"\n            manipulation_success = self.execute_manipulation(objects)\n            \n            return manipulation_success\n            \n        except Exception as e:\n            self.get_logger().error(f"Command execution failed: {e}")\n            return False\n'})}),"\n",(0,s.jsx)(n.h2,{id:"phase-8-testing-and-validation",children:"Phase 8: Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"81-individual-component-testing",children:"8.1 Individual Component Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test each component separately:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Voice processing: Verify Whisper transcription accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Planning: Test LLM response to various commands"}),"\n",(0,s.jsx)(n.li,{children:"Navigation: Validate path planning and obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Perception: Check object detection accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation: Test grasp execution"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"82-integrated-testing",children:"8.2 Integrated Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test the complete pipeline:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"End-to-end voice command execution"}),"\n",(0,s.jsx)(n.li,{children:"Error handling and recovery"}),"\n",(0,s.jsx)(n.li,{children:"Performance under various conditions"}),"\n",(0,s.jsx)(n.li,{children:"Safety mechanisms"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"83-scenario-testing",children:"8.3 Scenario Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test common scenarios:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'"Robot, please go to the kitchen and bring me the red cup from the table"'}),"\n",(0,s.jsx)(n.li,{children:'"Navigate to the living room and find the blue book, then put it on the shelf"'}),"\n",(0,s.jsx)(n.li,{children:'"Find the person near the window and follow them"'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Recognition Errors"}),": Ensure quiet environment and clear speech"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation Failures"}),": Verify map quality and localization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception Misses"}),": Check lighting conditions and sensor calibration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timing Issues"}),": Synchronize system clocks and message timestamps"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"debugging-strategies",children:"Debugging Strategies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Monitor all relevant topics\nros2 topic echo /voice_input\nros2 topic echo /natural_language_command\nros2 topic echo /execution_plan\nros2 topic echo /detected_objects\n\n# Check system status\nros2 run rqt_graph rqt_graph\nros2 lifecycle list\n\n# Monitor performance\nros2 run plotjuggler plotjuggler\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-real-time-performance",children:"1. Real-time Performance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize processing pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Use appropriate QoS settings"}),"\n",(0,s.jsx)(n.li,{children:"Implement message throttling where appropriate"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-resource-management",children:"2. Resource Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor CPU and memory usage"}),"\n",(0,s.jsx)(n.li,{children:"Implement lazy loading for large models"}),"\n",(0,s.jsx)(n.li,{children:"Use efficient data structures"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-communication-efficiency",children:"3. Communication Efficiency"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Minimize message size"}),"\n",(0,s.jsx)(n.li,{children:"Use appropriate buffering"}),"\n",(0,s.jsx)(n.li,{children:"Implement data compression where possible"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,s.jsx)(n.h3,{id:"technical-requirements",children:"Technical Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Successful execution of Voice \u2192 Plan \u2192 Navigate \u2192 Perceive \u2192 Manipulate pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Proper integration of components from all four modules"}),"\n",(0,s.jsx)(n.li,{children:"Robust error handling and system recovery"}),"\n",(0,s.jsx)(n.li,{children:"Adequate safety measures throughout the system"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Command execution success rate (>80%)"}),"\n",(0,s.jsx)(n.li,{children:"Average response time (<30 seconds for complete operation)"}),"\n",(0,s.jsx)(n.li,{children:"Perception accuracy (>90% for trained objects)"}),"\n",(0,s.jsx)(n.li,{children:"Navigation success rate (>95% in known environments)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Complete system architecture documentation"}),"\n",(0,s.jsx)(n.li,{children:"Code comments and API documentation"}),"\n",(0,s.jsx)(n.li,{children:"Testing procedures and results"}),"\n",(0,s.jsx)(n.li,{children:"Safety considerations and limitations"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"The capstone project demonstrates your mastery of Physical AI concepts by integrating all course modules into a functional robotic system. Focus on robust integration, error handling, and system-level thinking. Remember that this project showcases your ability to connect AI, perception, planning, and control components into a coherent system."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var t=i(6540);const s={},a=t.createContext(s);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);