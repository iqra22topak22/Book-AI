"use strict";(globalThis.webpackChunkphysical_ai_course=globalThis.webpackChunkphysical_ai_course||[]).push([[8170],{8453:(e,i,n)=>{n.d(i,{R:()=>t,x:()=>a});var l=n(6540);const s={},r=l.createContext(s);function t(e){const i=l.useContext(r);return l.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),l.createElement(r.Provider,{value:i},e.children)}},8611:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"module-4-vision-language-action/deliverables/deliverable-1","title":"Module 4 Deliverables: Vision-Language-Action","description":"Overview","source":"@site/docs/module-4-vision-language-action/deliverables/deliverable-1.md","sourceDirName":"module-4-vision-language-action/deliverables","slug":"/module-4-vision-language-action/deliverables/deliverable-1","permalink":"/docs/module-4-vision-language-action/deliverables/deliverable-1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vision-language-action/deliverables/deliverable-1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lab 4.1: Voice Command Processing with Whisper","permalink":"/docs/module-4-vision-language-action/labs/lab-1"},"next":{"title":"Capstone Project: Voice-Controlled Robot Assistant","permalink":"/docs/capstone-project"}}');var s=n(4848),r=n(8453);const t={},a="Module 4 Deliverables: Vision-Language-Action",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Deliverable 4.1: Voice Command Processing System",id:"deliverable-41-voice-command-processing-system",level:2},{value:"Requirements",id:"requirements",level:3},{value:"Submission Requirements",id:"submission-requirements",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:3},{value:"Deliverable 4.2: LLM-Based Planning System",id:"deliverable-42-llm-based-planning-system",level:2},{value:"Requirements",id:"requirements-1",level:3},{value:"Submission Requirements",id:"submission-requirements-1",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria-1",level:3},{value:"Deliverable 4.3: Complete VLA Integration",id:"deliverable-43-complete-vla-integration",level:2},{value:"Requirements",id:"requirements-2",level:3},{value:"Submission Requirements",id:"submission-requirements-2",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria-2",level:3},{value:"General Submission Guidelines",id:"general-submission-guidelines",level:2},{value:"Grading Rubric",id:"grading-rubric",level:2},{value:"Late Submission Policy",id:"late-submission-policy",level:2},{value:"Academic Integrity",id:"academic-integrity",level:2},{value:"Technical Requirements",id:"technical-requirements",level:2}];function d(e){const i={br:"br",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"module-4-deliverables-vision-language-action",children:"Module 4 Deliverables: Vision-Language-Action"})}),"\n",(0,s.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(i.p,{children:"This document outlines the deliverables for Module 4 of the Physical AI & Humanoid Robotics course. These deliverables demonstrate your understanding of Vision-Language-Action (VLA) systems, including voice processing with Whisper, LLM-based planning, and integration of all modalities for complex robotic tasks."}),"\n",(0,s.jsx)(i.h2,{id:"deliverable-41-voice-command-processing-system",children:"Deliverable 4.1: Voice Command Processing System"}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Due Date"}),": End of Week 10",(0,s.jsx)(i.br,{}),"\n",(0,s.jsx)(i.strong,{children:"Weight"}),": 15% of Module 4 grade"]}),"\n",(0,s.jsx)(i.h3,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsx)(i.p,{children:"Create a comprehensive voice command processing system using Whisper:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Implement Whisper-based speech-to-text with real-time processing capabilities"}),"\n",(0,s.jsx)(i.li,{children:"Design a robust natural language parser for robot commands"}),"\n",(0,s.jsx)(i.li,{children:"Integrate with ROS 2 for robot control"}),"\n",(0,s.jsx)(i.li,{children:"Include safety checks and validation for voice commands"}),"\n",(0,s.jsx)(i.li,{children:"Demonstrate system performance in various acoustic conditions"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"submission-requirements",children:"Submission Requirements"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Complete ROS 2 packages for voice processing"}),"\n",(0,s.jsx)(i.li,{children:"Configuration files for Whisper model selection"}),"\n",(0,s.jsx)(i.li,{children:"Audio preprocessing pipeline"}),"\n",(0,s.jsx)(i.li,{children:"Natural language command parser"}),"\n",(0,s.jsx)(i.li,{children:"Performance analysis and accuracy metrics"}),"\n",(0,s.jsx)(i.li,{children:"Technical documentation and user guide"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Whisper integration quality (25%)"}),"\n",(0,s.jsx)(i.li,{children:"Command parsing accuracy (25%)"}),"\n",(0,s.jsx)(i.li,{children:"Real-time performance (20%)"}),"\n",(0,s.jsx)(i.li,{children:"Safety and validation mechanisms (15%)"}),"\n",(0,s.jsx)(i.li,{children:"Documentation quality (15%)"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"deliverable-42-llm-based-planning-system",children:"Deliverable 4.2: LLM-Based Planning System"}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Due Date"}),": End of Week 11",(0,s.jsx)(i.br,{}),"\n",(0,s.jsx)(i.strong,{children:"Weight"}),": 20% of Module 4 grade"]}),"\n",(0,s.jsx)(i.h3,{id:"requirements-1",children:"Requirements"}),"\n",(0,s.jsx)(i.p,{children:"Develop an LLM-based planning system for robotic tasks:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Integrate a large language model (e.g., OpenAI GPT, HuggingFace models) for task planning"}),"\n",(0,s.jsx)(i.li,{children:"Create a context-aware system that considers robot capabilities and environment"}),"\n",(0,s.jsx)(i.li,{children:"Implement action decomposition from high-level goals"}),"\n",(0,s.jsx)(i.li,{children:"Include failure recovery and plan adaptation"}),"\n",(0,s.jsx)(i.li,{children:"Connect with perception and navigation systems"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"submission-requirements-1",children:"Submission Requirements"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"LLM integration and prompting framework"}),"\n",(0,s.jsx)(i.li,{children:"Task planning algorithms and heuristics"}),"\n",(0,s.jsx)(i.li,{children:"Context management system"}),"\n",(0,s.jsx)(i.li,{children:"Plan execution monitoring"}),"\n",(0,s.jsx)(i.li,{children:"Performance evaluation and comparison"}),"\n",(0,s.jsx)(i.li,{children:"Integration with robot systems"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"evaluation-criteria-1",children:"Evaluation Criteria"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Planning accuracy and feasibility (30%)"}),"\n",(0,s.jsx)(i.li,{children:"Context awareness and adaptation (25%)"}),"\n",(0,s.jsx)(i.li,{children:"Integration quality with robot systems (20%)"}),"\n",(0,s.jsx)(i.li,{children:"Failure recovery mechanisms (15%)"}),"\n",(0,s.jsx)(i.li,{children:"Performance optimization (10%)"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"deliverable-43-complete-vla-integration",children:"Deliverable 4.3: Complete VLA Integration"}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Due Date"}),": End of Week 12",(0,s.jsx)(i.br,{}),"\n",(0,s.jsx)(i.strong,{children:"Weight"}),": 25% of Module 4 grade"]}),"\n",(0,s.jsx)(i.h3,{id:"requirements-2",children:"Requirements"}),"\n",(0,s.jsx)(i.p,{children:"Integrate voice, language, and action systems for complete VLA functionality:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Implement the full workflow: Voice \u2192 Plan \u2192 Navigate \u2192 Perceive \u2192 Manipulate"}),"\n",(0,s.jsx)(i.li,{children:"Create a unified system architecture for VLA"}),"\n",(0,s.jsx)(i.li,{children:"Demonstrate complex multi-step tasks"}),"\n",(0,s.jsx)(i.li,{children:"Include error handling and recovery throughout the pipeline"}),"\n",(0,s.jsx)(i.li,{children:"Provide comprehensive evaluation of the integrated system"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"submission-requirements-2",children:"Submission Requirements"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Complete VLA system implementation"}),"\n",(0,s.jsx)(i.li,{children:"Multi-modal integration framework"}),"\n",(0,s.jsx)(i.li,{children:"Complex task demonstrations"}),"\n",(0,s.jsx)(i.li,{children:"Performance analysis across all modalities"}),"\n",(0,s.jsx)(i.li,{children:"Failure analysis and system robustness evaluation"}),"\n",(0,s.jsx)(i.li,{children:"Comprehensive technical documentation"}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"evaluation-criteria-2",children:"Evaluation Criteria"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"End-to-end workflow implementation (30%)"}),"\n",(0,s.jsx)(i.li,{children:"Multi-modal integration quality (25%)"}),"\n",(0,s.jsx)(i.li,{children:"Complex task execution success (20%)"}),"\n",(0,s.jsx)(i.li,{children:"System robustness and error handling (15%)"}),"\n",(0,s.jsx)(i.li,{children:"Documentation and analysis quality (10%)"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"general-submission-guidelines",children:"General Submission Guidelines"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"All code must be properly documented"}),"\n",(0,s.jsx)(i.li,{children:"Submit as a zipped package with clear directory structure"}),"\n",(0,s.jsx)(i.li,{children:"Include a README with build and run instructions"}),"\n",(0,s.jsx)(i.li,{children:"Use Python 3.10+ or C++17 as appropriate"}),"\n",(0,s.jsx)(i.li,{children:"Follow ROS 2 style guidelines (PEP 8 for Python, Google style for C++)"}),"\n",(0,s.jsx)(i.li,{children:"Provide video demonstrations where applicable"}),"\n",(0,s.jsx)(i.li,{children:"Include performance benchmarks and analysis"}),"\n",(0,s.jsx)(i.li,{children:"Document computing requirements and limitations"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Exceeds Expectations (A): All requirements met with advanced implementation"}),"\n",(0,s.jsx)(i.li,{children:"Meets Expectations (B): All requirements met with effective implementation"}),"\n",(0,s.jsx)(i.li,{children:"Approaches Expectations (C): Most requirements met with basic implementation"}),"\n",(0,s.jsx)(i.li,{children:"Below Expectations (D/F): Requirements not adequately met"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"late-submission-policy",children:"Late Submission Policy"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Within 24 hours: 10% grade reduction"}),"\n",(0,s.jsx)(i.li,{children:"Within 48 hours: 25% grade reduction"}),"\n",(0,s.jsx)(i.li,{children:"Beyond 48 hours: Not accepted without prior approval"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"academic-integrity",children:"Academic Integrity"}),"\n",(0,s.jsx)(i.p,{children:"All deliverables must be your own work. You may use online resources and documentation but must cite them appropriately. Code sharing with other students is not permitted."}),"\n",(0,s.jsx)(i.h2,{id:"technical-requirements",children:"Technical Requirements"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Computing resources for LLM inference (GPU recommended)"}),"\n",(0,s.jsx)(i.li,{children:"Integration with ROS 2 Humble Hawksbill"}),"\n",(0,s.jsx)(i.li,{children:"Whisper speech recognition capabilities"}),"\n",(0,s.jsx)(i.li,{children:"Audio input devices for voice processing"}),"\n",(0,s.jsx)(i.li,{children:"Compatible robot platform for end-to-end testing"}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);