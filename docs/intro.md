---
sidebar_position: 1
---

# Physical AI & Humanoid Robotics Course

## Course Overview

Welcome to the Physical AI & Humanoid Robotics course! This comprehensive 13-week program covers the fundamentals and advanced concepts of embodied intelligence, humanoid robotics, and AI integration using ROS 2, Gazebo, Unity, NVIDIA Isaac, and Vision-Language-Action systems.

### Course Structure

The course is organized into 4 core modules:
- **Module 1**: ROS 2 Nervous System (weeks 1-3)
- **Module 2**: Digital Twin (weeks 4-6)
- **Module 3**: NVIDIA Isaac Brain (weeks 7-9)
- **Module 4**: Vision-Language-Action (weeks 10-12)
- **Capstone Project**: Weeks 13

### Learning Outcomes

By the end of this course, students will be able to:
- Design and implement robotic systems using ROS 2
- Create digital twins with Gazebo and Unity
- Leverage NVIDIA Isaac for advanced robotics capabilities
- Integrate voice, vision, and action in robotic systems
- Implement a comprehensive capstone project

### Prerequisites

- Basic programming knowledge (Python)
- Understanding of linear algebra and calculus
- Familiarity with Linux command line

### Required Hardware

- RTX workstation for simulation
- Robot platform (Unitree or Hiwonder)
- Sensors (RealSense, IMU, etc.)

### Assessment

- Weekly labs and deliverables
- Module-based projects
- Comprehensive capstone project
- Final evaluation including the Voice → Plan → Navigate → Perceive → Manipulate workflow

## Course Modules

### Module 1: ROS 2 Nervous System

Covers the fundamental concepts of ROS 2 including nodes, topics, services, URDF, and the rclpy bridge.

### Module 2: Digital Twin

Focuses on creating simulation environments using Gazebo and Unity, with physics and sensor modeling.

### Module 3: NVIDIA Isaac Brain

Explores NVIDIA Isaac Sim, Isaac ROS, VSLAM, and Nav2 for advanced navigation and perception.

### Module 4: Vision-Language-Action

Integrates Whisper, LLM planning, and Vision-Language-Action systems for humanoid robots.

## Capstone Project

The culminating project integrates concepts from all modules, implementing the complete Voice → Plan → Navigate → Perceive → Manipulate workflow on a physical or simulated humanoid robot.